KIS: regression testing instant service for development on Linux kernel /(large open source software)

Given enough eyeballs, all bugs are shallow?
================================

Abstract
--------------------------------
  The distributed version control system, quick rolling development model, looseld coupled developers and complex hug code base give unprecedented difficulties and pressure to current linux kernel regression testing. This paper presented a preliminary study of characteristic of development, bug, patch and developers in linux kernel, and put forward an viewpoint: instead of enough eyeballs, the automated regression testing should be an instant service for kernel developers. An prototype of automated regression testing tool KIS was implementated and could check regression status of every new commits from thousands of kernel developers in **short time** (<1 hour) and test 30,000 kernels per day using 13 servers (??? 208 CPU cores and 416GB memroy ???). Our insights into Linux kernel developer behaviors and attitudes towards regression testing allow us to make KIS to provide **precise** bug reports, and to improve their usability and effectiveness **without any harassment**. In the linux kernel 3.7 development cycle, KIS provide 63 bug reports which were almost 11% of the total.




1.Introduction
--------------------------------
The kernel which forms the core of the Linux system is the result of one of the largest cooperative software
projects ever attempted. Regular 2-3 month releases deliver updates to Linux users, each with significant
new features, added device support, and improved performance. The rate of change in the kernel is high and
increasing, with between 8,000 and 12,000 patches going into each recent kernel release. These releases each
contain the work of over 1,000 developers representing nearly 200 corporations [Linux Kernel Development How Fast it is Going, Who is Doing It, What They are Doing, and Who is Sponsoring It 2012].  From the statistics in the current linux kernel 3.7 development cycle, 1,271 developers contributed to nearly 12,000 non-merge changesets, nearly 395,000 lines of code were removed, 719,000 lines that were added in 90 days???[Statistics from the 3.7 development cycle [LWN.net]]. But the regression  

å›¾è¡¨
http://www.gossamer-threads.com/lists/linux/

+2.6.1-* +patch* 
+2.6.1  +patch* 

+2.6.1-* +patch* +fix*  
+2.6.1 +patch* +fix* 

+3.1-* regression* bug* -patch* -fix*
+3.1 regression* bug* -patch* -fix*

https://groups.google.com/forum/?fromgroups#!searchin/linux.kernel/regression$20OR$20bug$202.6.0

1.2 Crrent status of testing

Because of the rapid development of Internet, there are lots of software systems that are built in a completely different manner, and have general developer/user community. These are not created from scratch in one fell swoop, but rather evolve from humble origins over a long period of time. 


1.3 example, the problems in developing linux kernel
history

The Linux data indicates that the kernel distribution grew by a
factor of 78.6 over 17(1/4) years â€“ an average annual compounded
growth rate of 28.8% (as measured by LoC). Portrayal of how a
software system grows is a central component of the perpetual
development model. â€œContinued growthâ€ is also Lehmanâ€™s 6th law
of software evolution (Lehman and Ramil, 1999). While not one
of the original three laws proposed in 1974, it nevertheless figures
prominently in the research literature. This may perhaps be
attributed to the fact that it is the easiest to measure directly. For
all these reasons, it is of interest to study the growth of Linux in
some detail.

The parallel development that is inherent in the perpetual development
model may also help to cope with radical changes to
the architecture. A project may branch off an exploratory architecture
development branch, that is developed in parallel and
independently from the main development backbone. If the new
architecture succeeds it will eventually be adopted, and other modules
will be ported to adhere to it. This is analogous to redundancy
in biological systems, which allows many mutations with no deleterious
effects on the organism, at the same time immediately
benefiting from advantageous mutations.

Development Process

Linux kernels are release in a 3 month cycle
 * First part is the 2week mergewindow
    * Linus merges new drivers, updates and so on
    * Release of a rc1 kernel marks end of the mergewindow
 * After rc1 is released only fixes are accepted
 * Every week a new release candidate
 * Somewhere after rc7 and rc9 the final kernel is released

It is easy to identify three phases in the Linux kernelâ€™s development
(as reflected in Fig. 2), which can be viewed as variations
on the more abstract and general model. The first is the version 1
kernels, from 1994 to 1996. During this period most of the activity
was in development, and only few maintenance updates were
made to production versions. The second phase, from 1996 to
2004, is characterized by the release of three long-lived production
versions (2.0, 2.2, and 2.4) that were maintained in parallel. Significant
developments were performed between these versions. This
led to big differences between them and long intervals between
their release dates which were extended even further by the protracted
release process itself. Such long intervals contradict the
desired rapid release cycle of open source software. Indeed, in the third phase (kernel version 2.6 since 2004)
production kernels are released regularly every 2â€“3 months. To
reduce the maintenance effort most of these are not maintained
much beyond the release of the next production version.

analysis of release in linux kernel

we can generalize an idealized release process
that includes the following set of decision points and activities
between them(Fig. 5):

1 Decide to release. The decision at this point in time is to stop
developing new functionality, and prepare to release what has
accumulated so far. Except for version 1.2, this is reflected by a
new pre-release series of kernel updates. The following activity is
one of stabilizing and improving the code, based on internal testing
by the developers. Additional development is done mainly to
fill holes in existing functionality, not to add new functionality,
so the rate of growth is expected to be reduced.

2. Perform the release. This is the actual point of the release itself,
where the new production kernel is released and the series of its
updates is started. The initial updates reflect a period of support
and followup, in which developers continue to stabilize
and improve the code, but now this is based on feedback and
bug-reports from actual users. Again, this may include some
additional development.

3. Fork a new development version. This decision signifies that the
released version is considered stable and usable. It is reflected
in the data by the forking of a new development version, which
will be pursued in parallel to the maintenance of the production
version that has just been released.

4. Discontinue maintenance. The decision to stop supporting a
production version is usually taken only after the subsequent
production version is well established. It does not mean that this
version will immediately cease to be used â€“ only that it will not
be further maintained, so no additional updates will be made.
In Linux the decision to cease maintenance is not necessarily
the final word, as distributors (such as Red Hat or Debian) may
continue to maintain a version that is important to them even
when it is no longer â€œofficiallyâ€ maintained.


Difference

This model of a release is different from the common software
release life cycle (Wikipedia, 2010 http://en.wikipedia.org/wiki/Software_release_life_cycle). In the common model, the prealpha
phase denotes development, and the alpha phase is testing.

In Linux and other
open-source software distributed on the Internet there is no manufacturing
step, and released software is immediately available.
But more importantly, support by the development team continues
after release. This followup phase, which continues until the
decision to fork a new development version, is missing from the
common model.

The idealized release model discussed above hinges upon the
decision to release.

In the major Linux production versions these
decisions were related to the development of a major feature, such
as multi-platform support and SMP support.However, such developments
took an order of two years to complete, causing major
delays in the release of other lesser features.


With version 2.6 it
was therefore decided to switch to a periodic scheme with a new
release around every 2â€“3 months. This also reduced the pressure
on developers, because if one release is missed the next one is not
far off.

A faster release cycle was problematic with the idealized release
model, as an underlying notion in that model is that the same people
alternate between development and release activities.With the
growth of Linux this became increasingly untrue.Core developers
became more involved with administrating the kernel versions,
while other developers more commonly contributed complete subsystems
that had been developed externally.At the same time,
a â€œstable teamâ€ was formed to take care of updates to previous
production versions. This enabled a three-way parallelization of
activities:

* Development of new functionality. This is done independently
by many developers in their own environment (importantly, this
is explicitly supported by the git model of distributed version
control, where each developer has his own private copy of the
codebase). Such development may take a long time, and is not
reflected in any way in the Linux kernel releases and updates
until it is merged during a convenient merge window.  æ—©ç‚¹ç»™è¿™äº›ç›¸å¯¹ç‹¬ç«‹çš„å¼€å‘è€…æä¾›testingæ”¯æŒï¼Œå¯ä»¥è®©æ–°åŠŸèƒ½æ›´å¿«æ›´å¥½åœ°merge åˆ°linux kernelä¸­
* Stabilization and testing. This is orchestrated by the core developers,
notably Linus Torvalds, in cooperation with the developers
who made contributions in the most recent merge window. It is
reflected as updates to the current rc version.
* Maintenance and updates of the previously released version,
reflected in updates to that version.
This parallelization allows the release cycle to be compressed
and completed within about 10 weeks.ã€fig8ã€‘

The life of a new version starts with a
merge window. This is a relatively short period (around two weeks)
where developers are invited to submit their new developments.ï¼ˆThese new features are expected to have been reviewed and signed off by subsystem
maintainers, and incorporated in the Linux â€œnextâ€ version, **but this is not
reflected in any official releases.**ï¼‰
When the merge window ends, a stabilization period (of around
two months) takes place. The updates of this not-yet-released new
version are called â€œrelease candidatesâ€ and designated by an â€œrcâ€
suffix. When the new version is considered stable, it is released. In
parallel, a new merge window is opened for the next version. The
released version is handed over to the stable team, which performs
maintenance updates (bug fixes and security patches) as needed.


In Linux, this refers to the releases of
the major production versions (1.2, 2.0, 2.2, and 2.4), and to the
â€œthird digitâ€ releases of the 2.6 versions starting with 2.6.11. As
we show, these two sets of releases are rather different from each
other.

Note that we do not refer to updates (or â€œminor releasesâ€) of
existing versions â€“ the third-digit releases before version 2.6.11,
and the fourth-digit releases of 2.6.11 and later. In Linux, such
updates are made when â€œenoughâ€ content accumulates or when
there is a new security patch that needs to be disseminated quickly.
This is a subjective decision made by whoever is responsible for the
version in question.


Importantly, Linuxâ€™s development defies common management
theory (Raymond, 2000). At the same time, it does not fit into
any common software lifecycle model. 

we note that Linux
development is a dynamic process in constant flux, and adapts to
overcome problems that occur as the system grows. Thus different
variants of the model apply to different parts of Linuxâ€™s evolution(software testing).

we emphasize the implications of continued
development and parallel branches,


All olde model we use this term exclusively to mean the
release of a new production version of the system â€“ an event sometimes
referred to as a â€œmajorâ€ release in daily development isn't analyzed by old papers.A notable example is the release of Windows Vista, where many
users elected not to upgrade but rather to keep using Windows XP.


This distinction between maintenance and continued
development, which is central to our model, is missing from
many discussions of maintenance,

Thus, while maintenance does
in fact apply to versions of the product that are in production use,
it is not the main activity but rather done in parallel to continued
development of the backbone. And from a terminology aspect,
maintenance is not a synonym for evolution.
æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦å…³æ³¨çƒ­ç‚¹ï¼ˆå³å½“å‰çš„develop é˜¶æ®µï¼‰

An interesting question that arises from the distinction between
continued development and maintenance is who performs these
activities.
maintainçš„æŠ•å…¥å’Œdevelopmentçš„æŠ•å…¥æ˜¯å¦æœ‰ä¸€ä¸ªé‡åŒ–çš„åˆ†æžï¼Ÿ

Real users doing real work are effectively brought into the development
loop. (Raymond, 2000). è¿™ä¸ªè§‚ç‚¹æœ‰é”™

The user requirements and the system that
solves them co-evolve together

The danger of releasing an unstable version is reduced, because
users upgrade to the new version only gradually, and they have
the previous version as a fallback. è¿™ä¸ªè§‚ç‚¹æœ‰é”™ï¼Œå› ä¸ºï¼Œbugè¶Šä¹…å­˜åœ¨ï¼Œè¶Šæ²¡æœ‰äººç†ä¼š
è¿™è®¸è¿™ä¸ªfeatureå°±æ²¡æœ‰äººé€‰æ‹©æˆ–ä½¿ç”¨äº†ã€‚    



In the context of open-source projects, maintainers and
developers may be the same people, as developers may have ownership
or at least a feeling of responsibility for their code, and
will therefore maintain it in parallel to performing other development
activities.


Production versions therefore
generally need to be maintained well beyond the release of the
next version. For example, Linux kernel version 2.4, which was first
released in January 2001, was still being maintained in late 2010.


In this paper we focus on the development life cicle of Linux kernel.The choice of Linux is not accidental. Linux is perhaps the bestknown
and most successful open-source project in the world, to
the point of becoming the poster-child of the whole open source
movement. The source code of its full development history is
freely available from http://www.kernel.org and numerous mirrors
worldwide â€“ at present totaling ??? versions(from 1991), ???? commits (from 2002) .

Importantly, Linuxâ€™s development defies common management
theory (Raymond, 2000). At the same time, it does not fit into
any common software lifecycle model. This motivates us to suggest
the perpetual development model. This model is descriptive
rather than prescriptive. Its goal is to create a framework for discussing
what is actually being done in the field and why, rather
than attempting to dictate ideals. In particular, we note that Linux
development is a dynamic process in constant flux, and adapts to
overcome problems that occur as the system grows. Thus different
variants of the model apply to different parts of Linuxâ€™s evolution.

Data from several recent Linux 2.6 releases is shown in Fig. 9.
Note that there is **no â€œdevelopment versionâ€** , and there are no
updates during the merge window. All development work is done
externally by the developers in their own environment, in parallel
to the release of previous versions. In few cases the first rc
update serves to extend the merge window, but in most cases the
rc updates do not contain any significant growth.

Importantly, the 2.6 releases are time-driven rather than being
content-driven. Merge windows are relatively short, and the time
to stabilize is also limited. If stabilization is not achieved, merged
functionality may be removed and delayed to the next version. This
leads to rapid dissemination of innovations and developments, but
also means that stable versions have a very short lifetime. As a
result, some versions are singled out for â€œlongtermâ€ maintenance,
and updated in parallel to subsequent releases.

bugå‘å±•è¶‹åŠ¿ï¼Œä¸è¿‡ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œå¯èƒ½ä¸Žå…¶ä»–ä»£ç æœ‰è”ç³»ï¼Œè€Œå…¶ä»–ä»£ç æ˜¯é¢‘ç¹å˜åŒ–çš„ï¼Œå¯¼è‡´bugè¶Šè€ï¼Œåˆ†æžbugåœ¨æ–°çš„ä»£ç ä¸Šæ˜¯å¦æˆç«‹éœ€è¦èŠ±å¾ˆå¤§çš„ç²¾åŠ›ï¼Œæˆ–è€…è¿™ä¸ªå·²ç»ä¸æ˜¯bugæˆ–è¢«fixæŽ‰äº†ã€‚

[Perpetual Development: A Model of the Linux Kernel Life Cycle] 2011

Given that the main changes in the compressed release model
result from merging new functionality during a merge window, one
may expect that the rate of growth will increase and that no code
will ever be removed. This is not the case. Data collected by Greg
Kroah-Hartman from the git repositories of versions 2.6.11 through
2.6.354 shows that thousands of lines are removed in each new
version. However, additions outnumber deletions by an average
factor of 2.12.

2. Our Goal and Challengs
--------------------------------

2.1 Goal

 æš‚æ—¶æ”¾åœ¨è¿™é‡Œ
development model

patch model

bug/regressing model

developer/tester model



2.2 Challenges

[KS2012: Kernel build/boot testing]2012
Furthermore, even when a build problem appears in a series of kernel commits but is later fixed before a mainline -rc release, this still creates a problem: developers performing bisects to discover the causes of other kernel bugs will encounter the build failures during the bisection process.

As Fengguang noted, the problem is that it takes some time for these regressions to be detected. By that time, it may be difficult to determine what kernel change caused the problem and who it should be reported to. Many such reports on the kernel mailing list get no response, since it can be hard to diagnose user-reported problems. Furthermore, the developer responsible for the problem may have moved on to other activities and may no longer be "hot" on the details of work that they did quite some time ago. As a result, there is duplicated effort and lost time as the affected developers resolve the problems themselves.

According to Fengguang, these sorts of regressions are an inevitable part of the development process. Even the best of kernel developers may sometimes fail to test for regressions. When such regressions occur, the best way to ensure they are resolved is to quickly and accurately determine the cause of the regression and promptly notify the developer who caused the regression.

ã€Testing for kernel performance regressionsã€‘2012

It is not uncommon for software projects â€” free or otherwise â€” to include a set of tests intended to detect regressions before they create problems for users. The kernel lacks such a set of tests. There are some good reasons for this; most kernel problems tend to be associated with a specific device or controller and nobody has anything close to a complete set of relevant hardware. So the kernel depends heavily on early testers to find problems. The development process is also, in the form of the stable trees, designed to collect fixes for problems found after a release and to get them to users quickly.

é—´éš”å¤ªä¹…äº†

Still, there are places where more formalized regression testing could be helpful. Your editor has, over the years, heard a large number of presentations given by large "enterprise" users of Linux. Many of them expressed the same complaint: they upgrade to a new kernel (often skipping several intermediate versions) and find that the performance of their workloads drops considerably. Somewhere over the course of a year or so of kernel development, something got slower and nobody noticed. Finding performance regressions can be hard; they often only show up in workloads that do not exist except behind several layers of obsessive corporate firewalls. But the fact that there is relatively little testing for such regressions going on cannot help.

æœºå™¨å˜åŒ–äº†

His results include a set of scheduler tests, consisting of the "starve," "hackbench," "pipetest," and "lmbench" benchmarks. On an Intel Core i7-based system, the results were generally quite good; he noted a regression in 3.0 that was subsequently fixed, and a regression in 3.4 that still exists, but, for the most part, the kernel has held up well (and even improved) for this particular set of benchmarks. At least, until one looks at the results for other processors. On a Pentium 4 system, various regressions came in late in the 2.6.x days, and things got a bit worse again through 3.3. On an AMD Phenom II system, numerous regressions have shown up in various 3.x kernels, with the result that performance as a whole is worse than it was back in 2.6.32.

å¯èƒ½çš„åŽŸå› 

Mel has a hypothesis for why things may be happening this way: core kernel developers tend to have access to the newest, fanciest processors and are using those systems for their testing. So the code naturally ends up being optimized for those processors, at the expense of the older systems. Arguably that is exactly what should be happening; kernel developers are working on code to run on tomorrow's systems, so that's where their focus should be. But users may not get flashy new hardware quite so quickly; they would undoubtedly appreciate it if their existing systems did not get slower with newer kernels.

He ran the sysbench tool on three different filesystems: ext3, ext4, and xfs. All of them showed some regressions over time, with the 3.1 and 3.2 kernels showing especially bad swapping performance. Thereafter, things started to improve, with the developers' focus on fixing writeback problems almost certainly being a part of that solution. But ext3 is still showing a lot of regressions, while ext4 and xfs have gotten a lot better. The ext3 filesystem is supposed to be in maintenance mode, so it's not surprising that it isn't advancing much. But there are a lot of deployed ext3 systems out there; until their owners feel confident in switching to ext4, it would be good if ext3 performance did not get worse over time.



ï¼Œå½“å›žå½’æµ‹è¯•å¯ä»¥è‡ªåŠ¨æ‰§è¡Œçš„æ—¶å€™ï¼Œç³»ç»Ÿå¯ä»¥è¾ƒå¿«åœ°åˆ°è¾¾ä¸€ä¸ªæ–°çš„ç¨³å®šçŠ¶æ€ï¼Œæ¯”å¦‚ä¸€ä¸ªç³»ç»Ÿå¦‚æžœæœ‰è‰¯å¥½çš„utåŸºç¡€ï¼Œé‚£ä¹ˆä»£ç çš„å˜æ›´ä¸€æ—¦æ£€å…¥ä»£ç åº“å½“ä¸­ï¼Œå°±å¯ä»¥è§¦å‘è¿žç»­æž„å»ºå·¥å…·è¿›è¡Œbuild and testï¼ˆå›žå½’æµ‹è¯•ï¼‰ï¼Œå¤±è´¥çš„ç»“æžœå°†åœ¨ç¬¬ä¸€æ—¶é—´é€šçŸ¥å˜æ›´ä»£ç çš„ç¨‹åºå‘˜ï¼Œæˆ–è€…é€šçŸ¥é…ç½®ç®¡ç†å‘˜ç«‹å³å¤„ç†æœ‰é—®é¢˜çš„ä»£ç ï¼Œè¿™ç§just-in-timeçš„å¤„ç†æ–¹å¼å°†å¤§å¤§å‡å°‘bugæ•°ç›®å¹¶æé«˜ä»£ç è´¨é‡ï¼ŒåŒæ—¶ä½¿å›¢é˜Ÿçš„åˆä½œå¼€å‘æ›´åŠ é¡ºç•…ã€‚æµ‹è¯•çš„è‡ªåŠ¨åŒ–çš„ç¨‹åº¦è¶Šé«˜ï¼Œå›žå½’çš„å‘¨æœŸå°±è¶ŠçŸ­ï¼Œæ•ˆæžœè¶Šæ˜Žæ˜¾ï¼Œè½¯ä»¶çš„è´¨é‡ä¹Ÿè¶Šé«˜ï¼Œæµ‹è¯•æ˜¯å¦è‡ªåŠ¨åŒ–ä¹Ÿæ˜¯å¼€å‘æ˜¯å¦æ•æ·çš„ä¸€ä¸ªä¸»è¦æ ‡å¿—ã€‚
ç”±äºŽæ•æ·æå€¡çŸ­è¿­ä»£å’Œé¢‘ç¹å‘å¸ƒï¼Œå› æ­¤ï¼Œè‡ªåŠ¨åŒ–çš„å›žå½’æµ‹è¯•æ˜¯å¿…é¡»çš„ï¼Œæ˜¯æ•æ·çš„é‡è¦æ ‡å¿—ã€‚

the analysis of model of Linux kernel open source software development




ææ—©æµ‹è¯•å…¶å®žä¹Ÿæœ‰é—®é¢˜

ã€http://lwn.net/Articles/269120/ã€‘

The question which was immediately raised was this: how do we deal with big API changes which require changes in multiple subsystems? These changes are already problematic, often requiring maintainers to rework their trees in the middle of the merge window. Trying to integrate such changes earlier, in a separate tree, could bring a new set of problems. There will be a lot of conflicts between patches done before and after the API change, and somebody is going to have to put the pieces back together again. Andrew does some of that now, but the problem is big enough that not even Andrew can solve it all the time. The bidirectional SCSI patches merged for 2.6.25 were held up as an example; that change required coordinated SCSI and block layer patches, and it never was possible to get the whole thing working in -mm.

è¡¥ä¸çš„å›°éš¾

äººä»¬å¯èƒ½å¾ˆæƒ³é€šè¿‡ä¸€ç³»åˆ—çš„è¡¥ä¸æ¥ä¸ºå†…æ ¸å¢žåŠ ä¸€ä¸ªå®Œæ•´çš„æ–°åŸºç¡€è®¾æ–½ï¼Œä½†ç›´åˆ°è¿™ä¸ªç³»åˆ—çš„æœ€åŽä¸€ä¸ªè¡¥ä¸ç”Ÿæ•ˆï¼Œè¿™ä¸ªæ–°è®¾æ–½æ‰å¥½ç”¨ã€‚è¿™ç§æƒ³æ³•åº”è¯¥å°½å¯èƒ½çš„é¿å…ï¼›å¦‚æžœè¿™ä¸€ç³»åˆ—è¡¥ä¸æ–°å¢žäº†regressionï¼ŒæŠ˜åŠé—®é¢˜æŸ¥æ‰¾æ–¹æ³•ä¼šæŠŠæœ€åŽä¸€ä¸ªè¡¥ä¸è§†ä¸ºå¯¼è‡´é—®é¢˜ç½ªé­ç¥¸é¦–ï¼Œå³ä½¿çœŸæ­£çš„bugå‘ç”Ÿåœ¨åˆ«å¤„ã€‚

æ­¤æ—¶è¿˜æœ‰å¯èƒ½å‘ç”Ÿçš„æ˜¯ä¸Žå…¶ä»–å¼€å‘è€…æ‰€å®Œæˆçš„å·¥ä½œçš„å†²çªï¼Œè¿™å–å†³äºŽä½ çš„è¡¥ä¸çš„æ€§è´¨ã€‚æœ€åçš„æƒ…å†µä¸‹ï¼Œä¸¥é‡çš„å†²çªå¯èƒ½å¯¼è‡´ä¸€äº›å·¥ä½œè¢«æ”¾åœ¨æ¬¡è¦çš„ä½ç½®ï¼Œè€Œå‰©ä¸‹çš„è¡¥ ä¸å¯èƒ½æœ€ç»ˆæˆåž‹å¹¶è¢«åˆå¹¶åˆ°å†…æ ¸ä¸­ã€‚å…¶ä»–æ—¶é—´ï¼Œå†²çªè§£å†³å°†æ¶‰åŠåˆ°ä¸Žå…¶ä»–å¼€å‘è€…ä¸€èµ·å·¥ä½œå¹¶ä¸”å¯èƒ½æ¶‰åŠåœ¨æºç æ ‘é—´ç§»åŠ¨è¡¥ä¸ä»¥ä¿è¯æ‰€æœ‰äº‹æƒ…éƒ½èƒ½æŒ‰è§„åˆ™åœ°åº”ç”¨ã€‚è¿™ç§ å·¥ä½œå¾ˆå¯èƒ½æ˜¯ä¸€ç§ç—›è‹¦ï¼Œä½†ä½ åº”è¯¥çŸ¥è¶³äº†ï¼š
åœ¨linux-nextæ ‘å‡ºçŽ°ä¹‹å‰ï¼Œè¿™äº›å†²çªå¾€å¾€åªæ˜¯åœ¨åˆå¹¶çª—å£æ‰“å¼€æœŸé—´å‡ºçŽ°ï¼Œä½ å¿…é¡»å°½å¿«å¤„ç†æŽ‰è¿™äº›å†²çªã€‚è€ŒçŽ° åœ¨ï¼Œåœ¨åˆå¹¶çª—å£æ‰“å¼€ä¹‹å‰å¼€å‘è€…å¯ä»¥ä»Žå®¹ä¸è¿«åœ°è§£å†³è¿™äº›å†²çªã€‚

é¦–å…ˆï¼Œä½ çš„è¡¥ä¸çš„å¯è§æ€§å·²ç»å†ä¸€æ¬¡å¢žåŠ äº†ã€‚ä½ å¯èƒ½ä¼šæ”¶åˆ°æ–°çš„ä¸€è½®è¯„å®¡æ„è§ï¼Œè¿™äº›æ„è§æ¥è‡ªäºŽé‚£äº›ä¹‹å‰ä¸æ›¾çŸ¥é“ä½ çš„è¡¥ä¸çš„å¼€å‘è€…ä»¬ã€‚ä½ å¾ˆå¯èƒ½å¿½ç•¥è¿™äº›æ„è§ï¼Œå›  ä¸ºå·²ç»æ²¡æœ‰å…³äºŽä»£ç åˆå¹¶çš„é—®é¢˜äº†ã€‚ä½†æŠµåˆ¶ä½è¿™ç§è¯±æƒ‘ï¼›å¯¹äºŽé‚£äº›æå‡ºé—®é¢˜æˆ–å»ºè®®çš„å¼€å‘è€…æ¥è¯´ï¼Œä½ ä»ç„¶éœ€è¦ä¿æŒå¤„äºŽç§¯æžç›¸åº”çš„çŠ¶æ€ã€‚

æœ€åçš„bugæŠ¥å‘Šç±»åž‹æ˜¯regressionã€‚å¦‚æžœä½ çš„è¡¥ä¸å¯¼è‡´äº†ä¸€ä¸ªregressionï¼Œä½ å°±ä¼šå‘çŽ°æœ‰è®¸å¤šåŒä»¤äººä¸èˆ’æœçš„çœ¼ç›åœ¨æ³¨è§†ç€ ä½ ï¼›regressionéœ€è¦è¢«å°½æ—©ä¿®å¤ã€‚å¦‚æžœä½ ä¸æƒ…æ„¿æˆ–æ²¡æœ‰èƒ½åŠ›ä¿®å¤è¿™ä¸ªregression(å¹¶ä¸”æ²¡æœ‰äººä¸ºä½ åšè¿™ä»¶äº‹)ï¼Œé‚£ä¹ˆåœ¨å†…æ ¸ç¨³å®šæœŸï¼Œä½ çš„è¡¥ä¸ å‡ ä¹Žå¯ä»¥è‚¯å®šä¼šè¢«ç§»é™¤ã€‚å› æ— æ³•ä¿®å¤regressionè€Œå¯¼è‡´è¡¥ä¸è¢«ä»Žå†…æ ¸æ‹‰å‡º(pull)ï¼Œé™¤äº†å¦å®šäº†ä¹‹å‰ä½ ä¸ºè¡¥ä¸è¿›å…¥ä¸»çº¿è€Œåšçš„æ‰€æœ‰å·¥ä½œä¹‹å¤–ï¼Œè¿˜å¾ˆå¯ èƒ½å¤§å¤§å¢žåŠ ä½ çš„åŽç»­å·¥ä½œè¢«åˆå¹¶çš„éš¾åº¦ã€‚

åœ¨å¤„ç†å®Œè‹¥å¹²regressionsä¹‹åŽï¼Œå¯èƒ½è¿˜æœ‰å¦å¤–ä¸€äº›æ™®é€šbugéœ€è¦å¤„ç†ã€‚å†…æ ¸ç¨³å®šæœŸæ˜¯ä¿®æ­£è¿™äº›bugä»¥åŠç¡®ä¿ä½ é¦–æ¬¡å‡ºçŽ°åœ¨æŸå†…æ ¸ä¸»çº¿ç‰ˆæœ¬çš„ä»£ç å°½ å¯èƒ½çš„ç¨³å®šå¯é çš„æœ€å¥½æ—¶æœºã€‚å› æ­¤ï¼Œè¯·å›žç­”bugæŠ¥å‘Šï¼Œå¹¶å°½å¯èƒ½åœ°ä¿®æ­£é—®é¢˜ã€‚å†…æ ¸ç¨³å®šæœŸå°±æ˜¯ä¸ºæ­¤è€Œè®¾ç«‹çš„ï¼›ä¸€æ—¦æ—§è¡¥ä¸ä¸­æ‰€æœ‰é—®é¢˜éƒ½è¢«å¤„ç†å®Œæ¯•ï¼Œä½ å°±èƒ½å¤Ÿå¼€å§‹ åˆ›å»ºä¸€äº›ç»å¦™çš„æ–°è¡¥ä¸äº†ã€‚


linux-nextçš„ä½œç”¨

[http://lwn.net/Articles/289013/]

Linux-next is a somewhat strange base on which to try to develop, though. It is built anew every day from over 100 subsystem trees, each of which can, itself, change from one day to the next. So linux-next is a moving target, just like the mainline is. But, unlike the mainline, linux-next has no consistent or coherent history. Every day's linux-next tree is a completely new creation with a unique - and transient - history.

 Rebasing completely rewrites the development history, causing the old history to disappear from the tree. So a patch series based on the previous history loses its foundation.

Another interesting aspect of linux-next development involves API changes. The longstanding rule in kernel development is that internal kernel interfaces can be changed if there is a good reason to do so, but that the person making the change is obligated to fix all in-tree code broken by that change. If an API change is introduced into linux-next, though, the developer is simply not able to fix any code which enters linux-next by way of the other subsystem trees. If the developer does get patches into those trees for the API change, they can no longer be built on top of kernels which lack that change - the mainline, for example. API changes have, in other words, become harder to do - a situation which some may see as a good thing.

What all this means is that API changes must be handled through techniques like the creation of backward-compatibility layers; those layers can then be removed a development cycle or two later once the transition is complete. Or changes can be split up and added to individual subsystem trees; that, however, can lead to interesting ordering dependencies between the trees. In some cases, we are seeing 2.6.27 changes being merged into 2.6.26 in stub form as a way of making all of the pieces fit together.

Then, there is the simple matter that developers like to have a stable base upon which to create their code. The linux-next tree, since it contains large amounts of relatively new code, will also contain its share of new bugs. That makes developers, who are often having enough trouble just tracking down their own bugs, somewhat grumpy. Development against the mainline tends to have a lower probability of forcing developers to look for bugs which are not of their own making.


[http://lwn.net/Articles/269225/]

Linux-next is a somewhat strange base on which to try to develop, though. It is built anew every day from over 100 subsystem trees, each of which can, itself, change from one day to the next. So linux-next is a moving target, just like the mainline is. But, unlike the mainline, linux-next has no consistent or coherent history. Every day's linux-next tree is a completely new creation with a unique - and transient - history.
A fully running and successful linux-next won't change outcomes a lot, I
expect.  It will help to pick up on obvious bugs and build errors and
bisection breakages prior to them hitting mainline.  But we fix those
things by -rc1 or -rc2 anyway.  So it's just a time-shift, causing no great
change in the final 2.6.x.

linux-next will do little to solve our (IMO) largest problem: unfixed
regressions and bugs.  otoh it will free up a lot of my time and
hair-tearing, so I can devote more attention to the bugs.  (where
"attention" usually= "sending irritiating emails")


linux-next çš„å·¥ä½œè¿‡ç¨‹

ã€http://lwn.net/Articles/287155/ã€‘

The idea behind this tree is relatively simple. Linux-next maintainer Stephen Rothwell keeps a list of trees (maintained with git or quilt) which are intended to be merged in the next development cycle. As of this writing, that list contains 95 trees, all full of patches aimed at 2.6.27. Once a day, Stephen goes through the process of applying these trees to the mainline, one at a time. With each merge, he looks for merge conflicts and build failures. The original plan for linux-next stated that trees causing conflicts or build failures would simply be dropped. In reality, so far, Stephen usually takes the time to figure out the problem; he'll then fix up or drop an individual patch to make everything fit again.

When this process is done, he releases the result as the linux-next tree for the day. Others then grab it and perform build testing on it; some people even boot and run the daily linux-next releases. All this results in a steady stream of problem reports, small fixes, patches moving from one tree to another, and so on - various bits of integration work required to make all of the pieces fit together nicely.

There is an interesting sort of implicit hierarchy in the ordering of the trees. Subsystem trees which are merged early in the process are less likely to run into conflicts than those which come later. When two trees do come into conflict, it's the owner of the later tree - the one which actually shows the conflict - who feels the most pressure to fix things up. The history so far, though, shows that there has been very little in the way of finger-pointing when conflicts arise, as they do almost every day. All of the developers understand that they are working on the same kernel, and they share a common interest in solving problems.

So, thus far, linux-next appears to be functioning as intended. It is serving as an integration point for the next kernel and helping to get many of the merging problems out of the way ahead of time. One aspect of this whole system remains untested, though: the movement of patches from linux-next into the mainline. As things stand now, there is no automatic movement between the trees; instead, maintainers will send their pull requests directly to Linus as always. If Linus refuses to merge certain trees, or if he merges them in an order different from their ordering in linux-next, integration problems could return. In the end, it seems like linux-next will have to drive the final integration process more than is anticipated now, but it will probably take a few development cycles to figure out how to make it all work.

Meanwhile, anybody who is interested in 2.6.27 can, to a great extent, run it now by grabbing linux-next. This tree has clarified one aspect of the development process: the 2-3 month "development cycle" run by Linus is, in fact, just the tip of the kernel development iceberg. It is the final integration and stabilization stage. Linux-next nearly doubles the length of the visible development cycle by assembling the next kernel long before Linus starts working on it. And even linux-next only comes into play toward the end of a patch's life.

Meanwhile, however, linux-next appears to have settled in as a long-term feature of the kernel development landscape. It is serving its purpose as a place to find and resolve integration problems; it has also had the effect of taking much of that integration work off of Andrew Morton's shoulders. And that, in turn, should free him to spend more time trying to get developers to fix all those bugs.

3. Priliminary Design and Implementation
--------------------------------
3.1 Arch and Components

3.2 Workflows

3.3 Optimizations

fast compile for large softs

[Reducing Build Time Through Precompilations for Evolving Large Software]
the longer the build process
(e.g., compilation and linking), the longer developers have
to wait in order to integrate their changes. Such problems
are exacerbated in sync-and-stabilize developments [10],
where program changes are integrated into the code-base
nightly as the product is built.
Concerning the efficiency in a build process, the declaration
redundancies slow down the compilation of individual
compilation unit where they occur; the false dependencies
require extra recompilation of the compilation units that do
not need to include the changed part.
This paper presents a precompilation (i.e. source-tosource
transformation before compilation) approach to the
removal of redundancies inside compilation units and false
dependencies among the files


 1) ç›®å½•çš„ snapshot æŠ€æœ¯å’ŒåŽ»é‡å­˜å‚¨æŠ€æœ¯ï¼Œç”¨æœ€å°çš„å­˜å‚¨å®¹é‡æ¥ä¿å­˜æ¯ä¸€ä¸ªåŽ†å²çŠ¶æ€ , 2) å’Œ p2p ä¼ è¾“ç±»ä¼¼çš„åˆ†å—ä¸”å¢žé‡ä¼ è¾“æŠ€æœ¯, 3ï¼‰æ— éœ€äººå·¥å¹²é¢„çš„è‡ªåŠ¨åŒ–åŒæ­¥ç®—æ³•ã€‚

å¼€å‘è€…åœ¨kerneldçš„ä¸åŒå¼€å‘é˜¶æ®µæœ‰ä¸åŒçš„ç‰¹ç‚¹ï¼ŒæŸé˜¶æ®µå¼€å‘äººå‘˜å¤šï¼Œä½†è¦æ±‚çš„æµ‹è¯•å°‘ï¼Œè¿™æ ·å¯ä»¥å¿«é€Ÿæä¾›ç®€å•æµ‹è¯•åˆ†æžç»™æ›´å¤šçš„äººã€‚æŸé˜¶æ®µmaintainerä¸ºä¸»ï¼ˆäººå°‘ï¼‰ï¼Œæµ‹è¯•å¤šï¼Œå¯ä»¥æ·±å…¥æµ‹è¯•ä¸€ä¸‹ã€‚ä½†å…¶å®žè¿™äº›é˜¶æ®µåœ¨æ—¶é—´ä¸Šæœ‰é‡å ã€‚

3.4 Priliminary Implementation

shell lang,  machine , hwo to run

Identify who to notify

Once you know the subsystem that is causing the issue, you should send a bug report. Some maintainers prefer bugs to be reported via the kernel.org Bugzilla, while others prefer that bugs be reported via the subsystem mailing list.

One of the most common questions you'll be asked when you report a bug is, "Can you reproduce your bug on the latest kernel?"

4. Preliminary Results
--------------------------------

4.1 performance of building kernel

4.2 performance of running kernel

4.3 the number of error every day

4.4 the accurate degree of error source 


5. Discussion and Future Directions
--------------------------------

æ¯”è¾ƒå…¶ä»–FLOSS communities.

6. Reference
--------------------------------
[http://en.wikipedia.org/wiki/Linux_kernel]

Development model


The current development model of the Linux kernel is such that Linus Torvalds makes the releases of new versions, also called the "vanilla" or "mainline" kernels, meaning that they contain the main, generic branch of development. This branch is officially released as a new version approximately every three months, after Torvalds does an initial round of integrating major changes made by all other programmers, and several rounds of bug-fix pre-releases.

In the current scheme, the main branch of development is not a traditional "stable" branch, instead it incorporates all kinds of changes, both the latest features as well as security and bug fixes. For users who do not want to risk updating to new versions containing code that may not be well tested, a separate set of "stable" branches exist, one for each released version, which are meant for people who just want the security and bug fixes, but not a whole new version. These branches are maintained by the stable team (Greg Kroah-Hartman, Chris Wright, maybe others).

Most Linux users use a kernel supplied by their Linux distribution. Some distributions ship the "vanilla" and/or "stable" kernels. However, several Linux distribution vendors (such as Red Hat and Debian) maintain another set of Linux kernel branches which are integrated into their products. These are by and large updated at a slower pace compared to the "vanilla" branch, and they usually include all fixes from the relevant "stable" branch, but at the same time they can also add support for drivers or features which had not been released in the "vanilla" version the distribution vendor started basing their branch from.

The development model for Linux 2.6 was a significant change from the development model for Linux 2.5. Previously there was a stable branch (2.4) where only relatively minor and safe changes were merged, and an unstable branch (2.5), where bigger changes and cleanups were allowed. Both of these branches had been maintained by the same set of people, led by Torvalds. This meant that users would always have a well-tested 2.4 version with the latest security and bug fixes to use, though they would have to wait for the features which went into the 2.5 branch. The downside of this was that the "stable" kernel ended up so far behind that it no longer supported recent hardware and lacked needed features. In the late 2.5.x series kernel some maintainers elected to try and back port their changes to the stable series kernel which resulted in bugs being introduced into the 2.4.x series kernel. The 2.5 branch was then eventually declared stable and renamed to 2.6. But instead of opening an unstable 2.7 branch, the kernel developers decided to continue putting major changes into the 2.6 branch, which would then be released at a pace faster than 2.4.x but slower than 2.5.x. This had the desirable effect of making new features more quickly available and getting more testing of the new code, which was added in smaller batches and easier to test.

As a response to the lack of a stable kernel tree where people could coordinate the collection of bug fixes as such, in December 2005 Adrian Bunk announced that he would keep releasing 2.6.16.y kernels when the stable team moved on to 2.6.17.[91] He also included some driver updates, making the maintenance of the 2.6.16 series very similar to the old rules for maintenance of a stable series such as 2.4.[92] Since then, the "stable team" had been formed, and it would keep updating kernel versions with bug fixes. In October 2008 Adrian Bunk announced that he will maintain 2.6.27 for a few years as a replacement of 2.6.16.[93] The stable team picked up on the idea[94] and as of 2010 they continue to maintain that version and release bug fixes for it, in addition to others.

After the change of the development model with 2.6.x, developers continued to want what one might call an unstable kernel tree, one that changes as rapidly as new patches come in. Andrew Morton decided to repurpose his -mm tree from memory management to serve as the destination for all new and experimental code. In September 2007 Morton decided to stop maintaining this tree.[95] In February 2008, Stephen Rothwell created the linux-next tree to serve as a place where patches aimed to be merged during the next development cycle are gathered.[96][97] Several subsystem maintainers also adopted the suffix -next for trees containing code which is meant to be submitted for inclusion in the next release cycle.

[edit]Maintenance
While Linus Torvalds supervises code changes and releases to the latest kernel versions, he has delegated the maintenance of older versions to other programmers.[98] Major releases as old as 2.0 (officially made obsolete with the kernel 2.2.0 release in January 1999) are maintained as needed, although at a very slow pace.

å†…æ ¸å¼€å‘è¿‡ç¨‹æ˜¯å¦‚ä½•è¿›è¡Œçš„

å†…æ ¸å¼€å‘è€…ä½¿ç”¨ä¸€ç§æ¾æ•£çš„åŸºäºŽæ—¶é—´çš„å‘å¸ƒ(release)è¿‡ç¨‹ï¼Œæ¯2åˆ°3ä¸ªæœˆå‘å¸ƒä¸€ä¸ªæ–°å†…æ ¸ç‰ˆæœ¬ã€‚

æ¯ä¸ª2.6.xå‘å¸ƒç‰ˆæœ¬éƒ½æ˜¯ä¸€ä¸ªä¸»è¦çš„å†…æ ¸å‘å¸ƒç‰ˆï¼Œå…¶ä¸­åŒ…å«äº†æ–°ç‰¹æ€§ã€å†…éƒ¨APIå˜åŒ–ä»¥åŠå…¶ä»–æ›´å¤šå†…å®¹ã€‚ä¸€ä¸ªå…¸åž‹çš„2.6å‘å¸ƒå¯èƒ½åŒ…æ‹¬è¶…è¿‡10000ä¸ªå˜ æ›´ä»¥åŠå‡ åä¸‡è¡Œä»£ç çš„æ”¹å˜ã€‚å› æ­¤2.6æ˜¯Linuxå†…æ ¸å¼€å‘çš„å‰æ²¿ï¼›å†…æ ¸é‡‡ç”¨ä¸€ç§æ»šåŠ¨å¼€å‘çš„æ¨¡åž‹ï¼ŒæŒç»­ä¸æ–­åœ°å°†é‡å¤§å˜åŒ–æ•´åˆè¿›æ¥ã€‚

å…³äºŽæ¯ä¸ªå‘è¡Œç‰ˆçš„è¡¥ä¸åˆå¹¶ï¼Œå¤§å®¶éµå¾ªä¸€ä¸ªç›¸å¯¹ç®€å•ç›´æŽ¥çš„è§„åˆ™ã€‚åœ¨æ¯è½®å†…æ ¸å¼€å‘å‘¨æœŸçš„å¼€å§‹ï¼Œæˆ‘ä»¬è¯´"åˆå¹¶çª—å£"æ˜¯æ‰“å¼€çš„ã€‚é‚£æ—¶ï¼Œé‚£äº›è¢«è®¤ä¸ºå·²ç»è¶³å¤Ÿç¨³å®š(å¹¶ ä¸”è¢«å¼€å‘ç¤¾åŒºæŽ¥å—)çš„ä»£ç ä¼šè¢«åˆå…¥ä¸»çº¿å†…æ ¸ã€‚åœ¨è¿™æœŸé—´ï¼Œå†…æ ¸å°†ä»¥æŽ¥è¿‘æ¯å¤©1000ä¸ªå˜åŒ–("è¡¥ä¸"æˆ–"å˜æ›´")çš„é€Ÿåº¦å°†ä¸€ä¸ªæ–°å¼€å‘å‘¨æœŸçš„å¤§é‡æ”¹å˜(ä»¥åŠæ‰€ æœ‰é‡å¤§çš„å˜åŒ–)åˆå…¥ä¸»çº¿å†…æ ¸ã€‚

(é¡ºä¾¿è¯´ä¸€å¥ï¼Œè¿™æ˜¯å€¼å¾—æ³¨æ„çš„æ˜¯åˆå¹¶çª—å£é˜¶æ®µæ•´åˆçš„å˜åŒ–ä¸æ˜¯å‡­ç©ºè€Œæ¥çš„ï¼›å®ƒä»¬æ—©å·²è¢«æ”¶é›†ã€æµ‹è¯•å’Œæäº¤åˆ°é˜¶æ®µæ ‘ä¸­çš„ã€‚è¿™ä¸ªè¿‡ç¨‹æ˜¯å¦‚ä½•è¿›è¡Œçš„åœ¨åŽç»­ä¼šæœ‰è¯¦ç»†ä»‹ç»)ã€‚

åˆå¹¶çª—å£æŒç»­æ‰“å¼€çº¦ä¸¤å‘¨æ—¶é—´ã€‚åœ¨è¿™æ®µæ—¶é—´çš„æœ«å°¾ï¼ŒLinus Torvaldsä¼šå®£å¸ƒåˆå¹¶çª—å£å…³é—­å¹¶å‘å¸ƒè¿™ä¸€è½®å†…æ ¸ç‰ˆæœ¬å¼€å‘çš„ç¬¬ä¸€ä¸ª"rc"ç‰ˆæœ¬(è¯‘æ³¨ï¼šRelease Candidateï¼Œå‘å¸ƒå€™é€‰ç‰ˆ)ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽç‰ˆæœ¬å·ç¡®å®šä¸º2.6.26çš„å†…æ ¸æ¥è¯´ï¼Œåˆå¹¶çª—å£å…³é—­åŽå‘å¸ƒçš„ç‰ˆæœ¬å°†ç§°ä¸º2.6.26-rc1ã€‚-rc1çš„å‘ å¸ƒæ˜¯ä¸€ä¸ªä¿¡å·ï¼Œé¢„ç¤ºç€åˆå¹¶æ–°ç‰¹æ€§çš„é˜¶æ®µå·²ç»è¿‡åŽ»äº†ï¼Œå¼€å‘å·¥ä½œè¿›å…¥äº†ç¨³å®šå†…æ ¸ç‰ˆæœ¬çš„é˜¶æ®µã€‚

åœ¨æŽ¥ä¸‹æ¥çš„6åˆ°10ä¸ªæ˜ŸæœŸé‡Œï¼Œåªæœ‰é‚£äº›ä¿®æ­£é—®é¢˜çš„è¡¥ä¸æ‰åº”è¯¥æäº¤åˆ°ä¸»çº¿ä¸­ã€‚å¶å°”ä¹Ÿä¼šæœ‰æŸä¸ªç‰¹åˆ«é‡è¦çš„å˜åŒ–è¢«å…è®¸æäº¤åˆ°ä¸»çº¿ï¼Œä½†è¿™ç§æƒ…å†µæžå…¶å°‘è§ï¼›é‚£äº›å°è¯• åœ¨åˆå¹¶çª—å£ä¹‹å¤–æäº¤æ–°ç‰¹æ€§çš„å¼€å‘è€…å¾€å¾€ä¼šæ”¶åˆ°ä¸€ä¸ªä¸å‹å¥½çš„å¯¹å¾…ã€‚ä½œä¸ºä¸€èˆ¬è§„åˆ™ï¼Œå¦‚æžœä½ çš„ç‰¹æ€§é”™è¿‡äº†åˆå¹¶çª—å£ï¼Œé‚£ä½ æœ€å¥½æ˜¯ç­‰å¾…ä¸‹ä¸€ä¸ªå¼€å‘å‘¨æœŸã€‚(ä¸€ä¸ªä¸å¸¸è§ çš„ä¾‹å¤–æ˜¯é‚£äº›ç”¨äºŽä¹‹å‰ä¸æ”¯æŒçš„ç¡¬ä»¶çš„é©±åŠ¨ç¨‹åºï¼›å¦‚æžœå®ƒä»¬æ²¡æœ‰è§¦åŠåˆ°æ ‘å†…ä»£ç ï¼Œé‚£ä¹ˆå®ƒä»¬å°±ä¸å¯èƒ½å¯¼è‡´å›žé€€(regression)ï¼Œåœ¨ä»»ä½•æ—¶å€™æ·»åŠ éƒ½åº”è¯¥æ˜¯ å®‰å…¨çš„ã€‚)

éšç€è¶Šæ¥è¶Šå¤šçš„ä¿®å¤è¿›å…¥ä¸»çº¿ï¼Œè¡¥ä¸çŽ‡å°†éšç€æ—¶é—´çš„æŽ¨ç§»è€Œä¸‹é™ã€‚Linuså¤§çº¦æ¯å‘¨å‘å¸ƒä¸€ä¸ªæ–°çš„-rcå†…æ ¸ç‰ˆæœ¬ï¼›åœ¨å†…æ ¸è¢«è®¤ä¸ºè¶³å¤Ÿç¨³å®šä¸”æœ€ç»ˆçš„2.6.xç‰ˆæœ¬å‘å¸ƒä¹‹å‰ï¼Œä¸€ä¸ªæ­£å¸¸ç³»åˆ—çš„ç‰ˆæœ¬å·ä¼šæ¼”è¿›åˆ°-rc6å’Œ-rc9ä¹‹é—´ã€‚ä¸€æ—¦ç‰ˆæœ¬ç¨³å®šä¸”æœ€ç»ˆå†…æ ¸ç‰ˆæœ¬å‘å¸ƒï¼Œæ•´ä¸ªè¿‡ç¨‹åˆå°†é‡æ–°å¼€å§‹ã€‚

    

å¯¹ regressionçš„ç†è§£

å¼€å‘äººå‘˜æ˜¯å¦‚ä½•åˆ¤æ–­ä½•æ—¶ç»“æŸè¿™ä¸€è½®çš„å¼€å‘å‘¨æœŸå¹¶å‘å¸ƒç¨³å®šç‰ˆå‘¢ï¼Ÿä»–ä»¬é‡‡ç”¨çš„æœ€é‡è¦çš„åº¦é‡æ–¹æ³•æ˜¯ä¸Šä¸€ä¸ªç‰ˆæœ¬çš„regressionåˆ—è¡¨ã€‚Bugè™½ç„¶æ˜¯ä¸å—æ¬¢è¿Ž çš„ï¼Œä½†é‚£äº›å­˜åœ¨äºŽä»¥å‰ç‰ˆæœ¬ä¸­çš„å¯å¯¼è‡´ç³»ç»Ÿå´©æºƒçš„é—®é¢˜åˆ™è¢«è®¤ä¸ºæ˜¯æ›´ä¸ºä¸¥é‡çš„ã€‚å› æ­¤ï¼Œé‚£äº›å¯¼è‡´å†…æ ¸å›žé€€çš„è¡¥ä¸å°†é­å—å†·é‡ï¼Œå¹¶ä¸”éžå¸¸å¯èƒ½åœ¨å†…æ ¸ç¨³å®šåŒ–é˜¶æ®µè¢«æ¢å¤ åˆ°åŽŸå…ˆçŠ¶æ€ã€‚

å¼€å‘è€…çš„ç›®æ ‡æ˜¯åœ¨ç¨³å®šç‰ˆå†…æ ¸å‘å¸ƒä¹‹å‰ä¿®æ­£æ‰€æœ‰å·²çŸ¥çš„regressionsã€‚ä½†åœ¨çŽ°å®žä¸–ç•Œä¸­ï¼Œè¦æƒ³è¾¾æˆè¿™ç§å®Œç¾Žçš„ç›®æ ‡ååˆ†å›°éš¾ï¼›è¿™ç§è§„æ¨¡çš„é¡¹ç›®å­˜åœ¨å¤ªå¤šçš„ å˜æ•°ã€‚æœ‰æŸä¸€ç‚¹å¯¼è‡´æœ€ç»ˆå‘å¸ƒæŽ¨è¿Ÿå°±ä¼šä½¿é—®é¢˜å˜å¾—æ›´åŠ ç³Ÿç³•ï¼›ç­‰å¾…ä¸‹ä¸€ä¸ªåˆå¹¶çª—å£çš„æ”¹å˜å°†ä¼šè¶Šæ¥è¶Šå¤šï¼Œå¹¶ä¸”ä¼šåœ¨ä¸‹ä¸€ä¸ªå‘¨æœŸå¯¼è‡´æ›´å¤šçš„regessionå‡ºçŽ°ã€‚ å› æ­¤ï¼Œå¤§å¤šæ•°2.6.xå†…æ ¸å‘å¸ƒæ—¶åªå¸¦æœ‰å°‘äº†çš„å·²çŸ¥regressionsï¼Œç„¶è€Œï¼Œä½†æ„¿è¿™äº›regressionséƒ½ä¸é‚£ä¹ˆä¸¥é‡ã€‚

ä¸€æ—¦ç¨³å®šç‰ˆå†…æ ¸å‘å¸ƒï¼Œå…¶åŽç»­çš„ç»´æŠ¤å·¥ä½œå°†äº¤ç”±"ç¨³å®šç‰ˆå°ç»„(stable team)"è¿›è¡Œï¼Œç›®å‰è¿™ä¸ªå°ç»„æˆå‘˜åŒ…æ‹¬Greg Kroah-Hartmanå’Œ Chris Wrightã€‚ç¨³å®šç‰ˆå°ç»„ä¼šä¸æ—¶åœ°å‘å¸ƒç¨³å®šç‰ˆå†…æ ¸çš„æ›´æ–°ç‰ˆæœ¬ï¼Œç‰ˆæœ¬å·é‡‡ç”¨2.6.x.yè¿™ç§æ•°å­—æ ·å¼ã€‚å¦‚æžœæƒ³è¦è¢«çº³å…¥æ›´æ–°ç‰ˆæœ¬ï¼Œè¡¥ä¸å¿…é¡»(1)ä¿®æ­£ä¸€ä¸ªé‡ è¦çš„bugå¹¶ä¸”(2)å·²ç»è¢«åˆå…¥ä¸‹ä¸€ä¸ªå†…æ ¸å¼€å‘ç‰ˆæœ¬çš„ä¸»çº¿ä¸­äº†ã€‚

ä¸€ä¸ªè¡¥ä¸çš„ç”Ÿå‘½å‘¨æœŸ

è¡¥ä¸ä¸ä¼šä»Žå¼€å‘è€…çš„é”®ç›˜ç›´æŽ¥è¿›å…¥å†…æ ¸ä¸»çº¿ã€‚ç›¸åï¼Œå¼€å‘ç¤¾åŒºè®¾è®¡äº†ä¸€ä¸ªç¨æ˜¾å¤æ‚çš„è¿‡ç¨‹(è™½ç„¶æœ‰äº›éžæ­£å¼)æ¥ä¿è¯æ¯ä¸ªè¡¥ä¸éƒ½èƒ½è¢«è¯„å®¡ä»¥ç¡®ä¿è´¨é‡ï¼Œä¸”æ¯ä¸ªè¡¥ä¸éƒ½ å®žçŽ°äº†ä¸€ä¸ªå¯¹å†…æ ¸ä¸»çº¿æœ‰å¸å¼•åŠ›çš„æ”¹å˜ã€‚å¯¹äºŽä¸€äº›è¾ƒå°çš„ä¿®æ­£æ¥è¯´ï¼Œè¿™ä¸ªè¿‡ç¨‹æ‰§è¡Œåœ°å¾ˆå¿«ï¼Œä½†å¯¹äºŽè¾ƒå¤§çš„ä¸”æœ‰äº‰è®®çš„æ”¹å˜æ¥è¯´ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½ä¼šæŒç»­æ•°å¹´ã€‚è®¸å¤šå¼€å‘ äººå‘˜æ‰€é­é‡çš„æŒ«æŠ˜éƒ½æ˜¯æ¥è‡ªäºŽç¼ºä¹å¯¹è¿™ä¸€è¿‡ç¨‹çš„ç†è§£æˆ–å°è¯•ç»•å¼€è¿™ä¸€è¿‡ç¨‹ã€‚

è¡¥ä¸çš„ç‰¹å¾

è¡¥ä¸å¿…é¡»æ˜¯ä¸ºæŸä¸€ä¸ªç‰¹å®šçš„å†…æ ¸ç‰ˆæœ¬è€Œå‡†å¤‡çš„ã€‚é€šå¸¸ï¼Œè¡¥ä¸åº”è¯¥åŸºäºŽlinusçš„gitæ ‘ä¸­çš„å½“å‰ä¸»çº¿ç‰ˆæœ¬ã€‚

åªæœ‰æœ€ç®€å•çš„æ”¹å˜æ‰åº”è¯¥ä»¥ä¸€ä¸ªå•ç‹¬çš„è¡¥ä¸å½¢å¼æä¾›ï¼›å…¶ä»–æ‰€æœ‰è¡¥ä¸éƒ½åº”è¯¥ç”±ä¸€ç³»åˆ—åˆç†çš„æ”¹å˜ç»„æˆã€‚

æ¯ä¸ªé€»è¾‘ä¸Šç‹¬ç«‹çš„æ”¹å˜éƒ½åº”è¯¥ä»¥ä¸€ä¸ªå•ç‹¬çš„è¡¥ä¸æä¾›ã€‚å¼€å‘è€…åªå¯¹ç¦»æ•£çš„ã€è‡ªå®Œå¤‡çš„æ”¹å˜æ„Ÿå…´è¶£ï¼Œè€Œä¸æ˜¯ä½ æä¾›çš„è¿™äº›æ”¹å˜çš„è·¯å¾„ä¿¡æ¯ã€‚è¿™äº›æ”¹å˜å¯å°("ä¸ºè¿™ä¸ªç»“æž„ä½“å¢žåŠ ä¸€ä¸ªå­—æ®µ")å¯å¤§(ä¾‹å¦‚ï¼Œå¢žåŠ ä¸€ä¸ªé‡è¦çš„æ–°é©±åŠ¨ç¨‹åº)ï¼Œä½†å®ƒä»¬éƒ½åº”è¯¥æ˜¯å°æ¦‚å¿µçš„å¹¶ä¸”å¯ç”¨ä¸€è¡Œæ–‡å­—æè¿°çš„ã€‚æ¯ä¸ªè¡¥ä¸æ‰€å¸¦æ¥çš„æ”¹å˜éƒ½åº”è¯¥å¯ä»¥è¢«ç‹¬ç«‹è¯„å®¡å’Œç‹¬ç«‹éªŒè¯çš„ã€‚

æ¯ä¸ªè¡¥ä¸éƒ½åº”è¯¥ç”Ÿäº§å‡ºä¸€ä¸ªå¯ä»¥æ­£ç¡®ç¼–è¯‘å’Œè¿è¡Œçš„å†…æ ¸ï¼›å³ä½¿ä½ çš„è¡¥ä¸åºåˆ—åœ¨ä¸­é—´è¢«æ‰“æ–­ï¼Œç»“æžœä»ç„¶åº”è¯¥æ˜¯ä¸€ä¸ªå¯å·¥ä½œçš„å†…æ ¸ã€‚éƒ¨åˆ†åº”ç”¨ä¸€ä¸ªè¡¥ä¸åºåˆ—æ˜¯ä¸€ç§å¸¸è§çš„æƒ…å†µï¼Œå°¤å…¶æ˜¯å½“"git bisect"å·¥å…·è¢«ç”¨äºŽæŸ¥æ‰¾regressionæ—¶ï¼›å¦‚æžœè¡¥ä¸åºåˆ—è¢«æ‰“æ–­çš„ç»“æžœæ˜¯ä¸€ä¸ªæŸåçš„å†…æ ¸ï¼Œåˆ™ä¼šå¯¼è‡´é‚£äº›å‚ä¸Žè¿½æŸ¥å†…æ ¸é—®é¢˜çš„å¼€å‘è€…å’Œç”¨æˆ·çš„å·¥ä½œæ›´åŠ å›°éš¾ã€‚


äººä»¬å¯èƒ½å¾ˆæƒ³é€šè¿‡ä¸€ç³»åˆ—çš„è¡¥ä¸æ¥ä¸ºå†…æ ¸å¢žåŠ ä¸€ä¸ªå®Œæ•´çš„æ–°åŸºç¡€è®¾æ–½ï¼Œä½†ç›´åˆ°è¿™ä¸ªç³»åˆ—çš„æœ€åŽä¸€ä¸ªè¡¥ä¸ç”Ÿæ•ˆï¼Œè¿™ä¸ªæ–°è®¾æ–½æ‰å¥½ç”¨ã€‚è¿™ç§æƒ³æ³•åº”è¯¥å°½å¯èƒ½çš„é¿å…ï¼›å¦‚æžœè¿™ä¸€ç³»åˆ—è¡¥ä¸æ–°å¢žäº†regressionï¼ŒæŠ˜åŠé—®é¢˜æŸ¥æ‰¾æ–¹æ³•ä¼šæŠŠæœ€åŽä¸€ä¸ªè¡¥ä¸è§†ä¸ºå¯¼è‡´é—®é¢˜ç½ªé­ç¥¸é¦–ï¼Œå³ä½¿çœŸæ­£çš„bugå‘ç”Ÿåœ¨åˆ«å¤„ã€‚æ‰€ä»¥æ— è®ºä½•æ—¶ï¼Œæ·»åŠ äº†æ–°ä»£ç çš„è¡¥ä¸éƒ½åº”è¯¥ä½¿å¾—é‚£äº›ä»£ç ç«‹å³å¯ç”¨ã€‚


some tools


In FLOSSMetrics, the information is retrieved automatically, thanks to the Blackbird 4 system
developed for it, which integrates some mining tools such as:
 CVSAnalY: stores in a database (MySQL or SQLite) the parsing of a log file from several
SCMs such as CVS, Subversion and Git.
 Mailing List Stats: parses information from mailing lists in mbox format and stores it in a
MySQL database.
 Bicho: retrieves information from Bugzilla BTS (so far, it works with KDE, GNOME and
Apache communities) and also the SourceForge tracker and stores it in a MySQL database.
 CMetrics, CCCC, PyMetrics, PerlMetrics: offer complexity metrics for several programming
languages.
 SLOCCOunt: developed by David Wheeler, provides basic information related to number
of SLOC and a basic COCOMO model.




äºŒã€è½¯ä»¶æµ‹è¯•åˆ†ç±»

  é»‘ç›’æµ‹è¯•ä¸Žç™½ç›’æµ‹è¯•â€”â€”æŒ‰æ˜¯å¦æŸ¥çœ‹æºä»£ç åˆ’åˆ†
          
é»‘ç›’æµ‹è¯•ï¼šæŠŠè¢«æµ‹çš„è½¯ä»¶çœ‹åšæ˜¯ä¸€ä¸ªé»‘ç›’å­ï¼Œä¸å…³å¿ƒç›’å­é‡Œé¢çš„ç»“æž„æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼Œåªå…³å¿ƒè½¯ä»¶çš„è¾“å…¥æ•°æ®å’Œè¾“å‡ºç»“æžœã€‚
          
ç™½ç›’æµ‹è¯•ï¼šæŠŠç›’å­ç›–æ‰“å¼€ï¼ŒåŽ»ç ”ç©¶é‡Œé¢çš„æºä»£ç å’Œç¨‹åºç»“æž„ï¼ŒæŸ¥çœ‹ç¨‹åºçš„æºä»£ç å…·ä½“æ˜¯æ€Žä¹ˆå®žçŽ°çš„ã€‚

é™æ€æµ‹è¯•ä¸ŽåŠ¨æ€æµ‹è¯•ï¼šâ€”â€”æŒ‰æ˜¯å¦è¿è¡Œç¨‹åºåˆ’åˆ†
          é™æ€æµ‹è¯•ï¼šstatic testing ä¸å®žé™…è¿è¡Œè¢«æµ‹è½¯ä»¶ï¼Œè€Œåªæ˜¯é™æ€çš„æ£€æŸ¥ç¨‹åºä»£ç ï¼ˆåŒ…æ‹¬æ£€æŸ¥æ³¨é‡Šï¼‰ï¼Œç•Œé¢æˆ–è€…æ–‡æ¡£ä¸­å¯èƒ½å­˜åœ¨çš„é”™è¯¯çš„è¿‡ç¨‹ã€‚
          åŠ¨æ€æµ‹è¯•ï¼šdynamic testing å®žé™…è¿è¡Œè¢«æµ‹ç¨‹åºï¼Œè¾“å…¥ç›¸åº”çš„æµ‹è¯•æ•°æ®ï¼Œæ£€æŸ¥å®žé™…è¾“å‡ºç»“æžœå’Œé¢„æœŸç»“æžœæ˜¯å¦ç›¸ç¬¦ã€‚
         åˆ¤æ–­ä¸€ä¸ªæµ‹è¯•å±žäºŽåŠ¨æ€æµ‹è¯•è¿˜æ˜¯é™æ€æµ‹è¯•ï¼Œå”¯ä¸€çš„æ ‡å‡†æ˜¯çœ‹æ˜¯å¦è¿è¡Œç¨‹åºã€‚

å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€ç³»ç»Ÿæµ‹è¯•ã€éªŒæ”¶æµ‹è¯•ï¼šâ€”â€”æŒ‰é˜¶æ®µåˆ’åˆ†
          æ—¶é—´æ¯”ä¾‹1:2:4:2
          
å•å…ƒæµ‹è¯•ï¼šunit testing å¯¹è½¯ä»¶ä¸­çš„æœ€å°å¯æµ‹å•å…ƒè¿›è¡Œæ£€æŸ¥å’ŒéªŒè¯ï¼ˆå•å…ƒæ˜¯æŒ‡è®¤ä¸ºè§„å®šçš„æœ€å°è¢«æµ‹åŠŸèƒ½æ¨¡å—ï¼‰
                          ä¾æ®ï¼šæºç¨‹åºï¼Œã€Šè¯¦ç»†è®¾è®¡æ–‡æ¡£ã€‹
                          ä¸€èˆ¬æ­¥éª¤ï¼š1.ç¼–è¯‘è¿è¡Œç¨‹åº
                                           2.é™æ€æµ‹è¯•ï¼ˆæ£€æŸ¥ä»£ç æ˜¯å¦ç¬¦åˆè§„èŒƒï¼‰
                                           3.åŠ¨æ€æµ‹è¯•
                          æ¡©æ¨¡å—å’Œé©±åŠ¨æ¨¡å—
                                   æ¡©æ¨¡å—ï¼šstub æ¨¡æ‹Ÿè¢«æµ‹æ¨¡å—æ‰€è°ƒç”¨çš„æ¨¡å—
                                    é©±åŠ¨æ¨¡å—ï¼šdriver ç”¨æ¥æŽ¥æ”¶æµ‹è¯•æ•°æ®ï¼Œå¯åŠ¨è¢«æµ‹æ¨¡å—å¹¶è¾“å‡ºç»“æžœ
          
é›†æˆæµ‹è¯•ï¼šintegration testing å•å…ƒæµ‹è¯•çš„ä¸‹ä¸€é˜¶æ®µï¼Œå°†é€šè¿‡æµ‹è¯•çš„å•å…ƒæ¨¡å—ç»„è£…æˆç³»ç»Ÿæˆ–å­ç³»ç»Ÿï¼Œå†è¿›è¡Œæµ‹è¯•ï¼Œé‡ç‚¹æµ‹è¯•ä¸åŒæ¨¡å—çš„æŽ¥å£ï¼Œæ£€æŸ¥å„ä¸ªå•å…ƒæ¨¡å—ç»“åˆåˆ°ä¸€èµ·èƒ½å¦ååŒé…åˆï¼Œæ­£å¸¸è¿è¡Œã€‚
                         ä¾æ®ï¼šå•å…ƒæµ‹è¯•çš„æ¨¡å—ä»¥åŠã€Šæ¦‚è¦è®¾è®¡æ–‡æ¡£ã€‹
          
ç³»ç»Ÿæµ‹è¯•ï¼šsystem testing å°†æ•´ä¸ªè½¯ä»¶ç³»ç»Ÿçœ‹åšä¸€ä¸ªæ•´ä½“è¿›è¡Œæµ‹è¯•ï¼ŒåŒ…æ‹¬å¯¹åŠŸèƒ½ï¼Œæ€§èƒ½ï¼Œä»¥åŠè½¯ä»¶æ‰€è¿è¡Œçš„è½¯ç¡¬ä»¶çŽ¯å¢ƒè¿›è¡Œæµ‹è¯•ã€‚
                          éœ€è¦èŠ±å¤§é‡æ—¶é—´å’Œç²¾åŠ›åŽ»å®Œæˆçš„ï¼Œä¹Ÿæ˜¯è½¯ä»¶äº¤ç»™ç”¨æˆ·è¿›è¡ŒéªŒæ”¶æµ‹è¯•çš„æœ€åŽä¸€é“å…³å¡ã€‚
                         ä¾æ®ï¼šã€Šç³»ç»Ÿéœ€æ±‚è§„æ ¼è¯´æ˜Žä¹¦ã€‹
           
éªŒæ”¶æµ‹è¯•ï¼šacceptance testing ç³»ç»Ÿæµ‹è¯•çš„åŽæœŸï¼Œä»¥ç”¨æˆ·æµ‹è¯•ä¸ºä¸»ï¼Œæˆ–æœ‰æµ‹è¯•äººå‘˜ç­‰è´¨é‡ä¿éšœäººå‘˜å…±åŒå‚ä¸Žçš„æµ‹è¯•ã€‚
                              åˆ†ä¸º aï¼ˆa faï¼‰æµ‹è¯• ã€ç”±ç”¨æˆ·ï¼Œæµ‹è¯•äººå‘˜ï¼Œå¼€å‘äººå‘˜ç­‰å…±åŒå‚ä¸Žçš„å†…éƒ¨æµ‹è¯•ã€‘å’Œ b (be ta)æµ‹è¯•ã€å†…æµ‹åŽçš„å…¬æµ‹ï¼Œå®Œå…¨äº¤ç»™æœ€ç»ˆç”¨æˆ·çš„æµ‹è¯•ã€‘

åŠŸèƒ½æµ‹è¯•å’Œæ€§èƒ½æµ‹è¯•ï¼šâ€”â€”å±žäºŽé»‘ç›’æµ‹è¯•
         
 åŠŸèƒ½æµ‹è¯•ï¼šfunction testing  æ£€æŸ¥å®žé™…è½¯ä»¶çš„åŠŸèƒ½æ˜¯å¦ç¬¦åˆç”¨æˆ·çš„éœ€æ±‚
                         åŠŸèƒ½æµ‹è¯•åˆå¯ä»¥ç»†åˆ†ä¸ºå¾ˆå¤šç§ï¼šé€»è¾‘åŠŸèƒ½æµ‹è¯•logic function testingï¼Œç•Œé¢æµ‹è¯•UI testingï¼Œæ˜“ç”¨æ€§æµ‹è¯•usabilityï¼Œå®‰è£…æµ‹è¯•installationï¼Œå…¼å®¹æ€§æµ‹   è¯•compatibilityç­‰ã€‚
        
  æ€§èƒ½æµ‹è¯•ï¼šperformance testing ä¸€èˆ¬è¦ä½¿ç”¨è‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·ã€‚è½¯ä»¶æ€§èƒ½ä¸»è¦åŒ…æ‹¬æ—¶é—´æ€§èƒ½å’Œç©ºé—´æ€§èƒ½ã€‚
                           æ€§èƒ½æµ‹è¯•åˆ†ä¸ºï¼šä¸€èˆ¬æ€§èƒ½æµ‹è¯•ï¼Œç¨³å®šæ€§æµ‹è¯•ï¼Œè´Ÿè½½æµ‹è¯•ï¼ŒåŽ‹åŠ›æµ‹è¯•
                                   ä¸€èˆ¬æ€§èƒ½æµ‹è¯•ï¼šè®©è¢«æµ‹ç³»ç»Ÿåœ¨æ­£å¸¸çš„è½¯ç¡¬ä»¶çŽ¯å¢ƒä¸‹è¿è¡Œï¼Œä¸å‘å…¶æ–½åŠ ä»»ä½•åŽ‹åŠ›çš„æ€§èƒ½æµ‹è¯•ã€‚
                                   ç¨³å®šæ€§æµ‹è¯•ï¼šreliability ä¹Ÿå«åšå¯é æ€§æµ‹è¯•ã€‚è¿žç»­è¿è¡Œè¢«æµ‹ç³»ç»Ÿï¼Œæ£€æŸ¥ç³»ç»Ÿè¿è¡Œæ—¶çš„ç¨³å®šç¨‹åº¦ã€‚
                                   è´Ÿè½½æµ‹è¯•ï¼šload è®©è¢«æµ‹ç³»ç»Ÿåœ¨å…¶èƒ½å¿å—çš„åŽ‹åŠ›çš„æžé™èŒƒå›´ä¹‹å†…è¿žç»­è¿è¡Œï¼Œæ¥æµ‹è¯•ç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚
                                   åŽ‹åŠ›æµ‹è¯•ï¼šstress  æŒç»­ä¸æ–­çš„ç»™è¢«æµ‹ç³»ç»Ÿå¢žåŠ åŽ‹åŠ›ï¼Œåªè¦å°†è¢«æµ‹ç³»ç»ŸåŽ‹åž®ä¸ºæ­¢ï¼Œç”¨æ¥æµ‹è¯•ç³»ç»Ÿæ‰€èƒ½æ‰¿å—çš„æœ€å¤§åŽ‹åŠ›ã€‚

å›žå½’æµ‹è¯•ï¼Œå†’çƒŸæµ‹è¯•ï¼Œéšæœºæµ‹è¯•ï¼š
          
å›žå½’æµ‹è¯•ï¼šregression å¯¹è½¯ä»¶çš„æ–°çš„ç‰ˆæœ¬æµ‹è¯•æ—¶ï¼Œé‡å¤æ‰§è¡Œä¸Šä¸€ä¸ªç‰ˆæœ¬æµ‹è¯•æ—¶çš„ç”¨ä¾‹ã€‚

          
å†’çƒŸæµ‹è¯•ï¼šsmoke æ˜¯æŒ‡å¯¹ä¸€ä¸ªæ–°ç‰ˆæœ¬è¿›è¡Œç³»ç»Ÿå¤§è§„æ¨¡çš„æµ‹è¯•ä¹‹å‰ï¼Œå…ˆéªŒè¯ä¸€ä¸‹è½¯ä»¶çš„åŸºæœ¬åŠŸèƒ½æ˜¯å¦å®žçŽ°ï¼Œæ˜¯å¦å…·å¤‡å¯æµ‹æ€§ã€‚
          
éšæœºæµ‹è¯•ï¼šrandom æµ‹è¯•ä¸­æ‰€æœ‰çš„è¾“å…¥æ•°æ®éƒ½æ˜¯éšæœºç”Ÿæˆçš„ï¼Œå…¶ç›®çš„æ˜¯æ¨¡æ‹Ÿç”¨æˆ·çš„çœŸæ˜¯æ“ä½œï¼Œå¹¶å‘çŽ°ä¸€äº›è¾¹ç¼˜æ€§çš„é”™è¯¯

ä¸‰ã€è½¯ä»¶æµ‹è¯•åŸºæœ¬åŽŸåˆ™
        1.zero bug å’Œgood enoughåŽŸåˆ™
               zero bug ï¼š è½¯ä»¶æ²¡æœ‰ä»»ä½•çš„é”™è¯¯
               good enoughï¼šåªè¦è½¯ä»¶è¾¾åˆ°ä¸€å®šçš„è´¨é‡è¦æ±‚ï¼Œå°±å¯ä»¥åœæ­¢æµ‹è¯•äº†ã€‚ï¼ˆè¿‡åˆ†çš„æµ‹è¯•æµªè´¹èµ„æºï¼‰
          2.ä¸è¦è¯•å›¾ç©·ä¸¾æµ‹è¯•
          3.å¼€å‘äººå‘˜ä¸èƒ½æ—¢æ˜¯è¿åŠ¨å‘˜åˆæ˜¯è£åˆ¤
          4.è½¯ä»¶æµ‹è¯•è¦å°½æ—©æ‰§è¡Œ
          5.è½¯ä»¶æµ‹è¯•åº”è¯¥è¿½æº¯éœ€æ±‚
          6.ç¼ºé™·çš„äºŒå…«å®šç†
               ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè½¯ä»¶80%çš„ç¼ºé™·é›†ä¸­åœ¨20%çš„æ¨¡å—
          7.ç¼ºé™·å…·æœ‰å…ç–«æ€§



ã€Tracking of Linux Kernel Regressionsã€‘

In recent years the tracking of kernel regressions has become an important part of the kernel development
process for two basic reasons. First, it tells developers which bugs should be taken care of rst so that they
know what to focus on. Second, it gives Linus an idea about how many things should ideally be xed before
the next major release of the kernel so that the users for whom the previous kernels worked correctly aren't
disappointed.

It is done in such a way that the entire history record associated with it is preserved in the kernel Bugzilla
and can be extracted from there at any time. Among other things, this allows one to produce statistics similar
to the ones presented in the previous sections and to get some insight into the testers' and developers' activities
related to the listed regressions.


[1] Linux: Releasing With Known Regressions (http://kerneltrap.org/node/8110).
[2] Linux: Tracking Regressions (http://kerneltrap.org/node/8241).
[3] Radioactive decay (http://en.wikipedia.org/wiki/Radioactive_decay).


ã€http://kerneltrap.org/node/8110ã€‘ in 2007 
éœ€è¦ä»”ç»†é˜…è¯»
There is a conflict between Linus trying to release kernels every
2 months and releasing with few regressions.

(I've said this before, but I'll say it again: one thing that would 
already make bugzilla better is to just always drop any bug reports that 
are more than a week old and haven't been touched. It wouldn't need *much* 
touching, but if a reporter cannot be bothered to say "still true with 
current snapshot" once a week, then it shouldn't be seen as being somehow 
up to those scare resources we call "developers" to have to go through 
it).

And almost all hard bugs are about hardware interactions. Drivers. Big 
iron. Things like that - ie unlike something like a compiler, you can 
seldom say "this test-case crashes". Yes, that does happen for the kernel 
too, but those are the *easy* bugs. Those generally get fixed in a day or 
two.


[KS2011: Development process issues]

Dave Airlie complained about the number of patches that break the kernel and make bisection of problems during the merge window impossible. Linus acknowledges the problem, saying that he wishes every tree that feeds into the kernel would be bisectable, but that is never going to happen.

That is especially important for problem reporters who are not kernel developers and who cannot be expected to cope with kernels that explode in the middle of a bisection series. 

[KS2012: Improving development processes: linux-next [LWN.net]]

In response to the question of how much work was required to maintain linux-next, the maintainer, Stephen Rothwell, said it required between four and ten hours per day, depending on the stage in the kernel-release cycle. In the end



[KS2012: Improving development processes: linux-next [LWN.net]]
need 5MB more memory
such as kernel panics will generate a call trace that includes source file names and line numbers: 

    Call to panic() with the patch set
    ----------------------------------
    Call Trace:
     [<ffffffff815f3003>] panic+0xbd/0x14 panic.c:111
     [<ffffffff815f31f4>] ? printk+0x68/0xd printk.c:765
     [<ffffffffa0000175>] panic_write+0x25/0x30 [test_panic] test_panic.c:189
     [<ffffffff8118aa96>] proc_file_write+0x76/0x21 generic.c:226
     [<ffffffff8118aa20>] ? __proc_create+0x130/0x21 generic.c:211
     [<ffffffff81185678>] proc_reg_write+0x88/0x21 inode.c:218
     [<ffffffff81125718>] vfs_write+0xc8/0x20 read_write.c:435
     [<ffffffff811258d1>] sys_write+0x51/0x19 read_write.c:457
     [<ffffffff815f84d9>] ia32_do_call+0x13/0xc ia32entry.S:427

helpfull for git bisection

The improved call-tracing information that is provided by these patches would undoubtedly make life somewhat easier for diagnosing the causes of some kernel crashes. However, there is a cost: the memory footprint of the resulting kernel is much larger. During the session, a figure of 20 MB was mentioned,although in a mail that he later sent to the kernel summit discussion list, Jason clarified that the figure was more like 10 MB. 

Linus then weighed in to argue against the proposal altogether. In his view, kernel panics are a small enough part of user bug reports that the cost of this approach is unwarranted; an overhead of something like 1 MB for the increase in memory footprint would be more reasonable, he thought. Linus further opined that one can, with some effort, obtain similar traceback information by loading the kernel into GDB. 

Although Jason's proposed patches provide some helpful debugging functionality, the approach met enough negative response that it seems unlikely to be merged in anything like its current form. However, Jason may not be ready to completely give up on the idea yet. In his mail sent soon after the session, he hypothesized about some modifications to his approach that might bring the memory footprint of his feature down to something on the order of 5MB, as well as other approaches that could be employed so that the end user had greater control over when and if this feature was deployed for a running kernel. Thus, it may be that we'll see this idea reappear in a different form at a later date.

[KS2012: Stable kernel management [LWN.net]]

A few people wanted to understand more clearly the criteria that determine whether a patch should be sent for the stable series, and others noted that there seemed to be some latitude as to what Greg considered to be an acceptable patch


 the summary rationale for stable: if the patch would be of interest for distributions aiming to produce a stable kernel for a distribution release, then that patch should be submitted to stable. 

They are regressions under some specific workload that a user has, or with specific hardware (or combinations of hardware) that a user has.
As a result, it's impossible for any single test project to have comprehensive coverage.

The kernel regression tracker (and assistants) are not people running the tests, they are people working with the users who run into problems, helping those users identify the relevant factors of their environment/workload, identifying where the problem started, helping get the report in front of the appropriate maintainer, and then tracking it to keep it from getting lost (and hopefully the user doesn't disappear in the middle of all this), with an additional task of trying to combine duplicate reports (which gains reliability in terms of the user reporting)

If it's a workload related problem, once a problem workload can be simulated, the maintainer/developer may be able to go off and work on it without needing the user to test it all the time, but the user is still needed to test the resulting fix because the simulated workload may not match the real workload as closely as everyone thinks.

If it's a hardware related problem, it requires someone with the appropriate hardware to test the result (and if it's a combination of hardware it's even worse), the maintainers and developers cannot have every variant of hardware, so it's impossible for them to test, and so when they think they have the fix, they will again need to go back to the user to validate the fix.

[KS2008: Kernel quality and release process]
Rafael also noted that some regressions attract no debugging effort at all; it seems that nobody is interested in working on them. It can be disheartening for users to hear nothing about a reported regression at all

He also noted that regressions which have been bisected (to identify the change which first caused the problem to happen) tend to get fixed much more quickly.

The data from the bisection is undoubtedly useful, but the real benefit probably comes from fingering the guilty party, who then feels the need to get a fix in place.


Another thing Rafael pointed out is that we have a small core of dedicated testers; most of our regressions are reported by a small, recurring group of people. Perhaps we could recruit some of those people to help with the management of bugs. They could track reports, get more information from users, and harass maintainers to get fixes in place. These people have already shown a certain amount of dedication; giving them this kind of role would let them expand the help they are able to give to the kernel community.


[KS2009: Regressions]
Looking at things that way, Rafael says, regressions have a half life of about 17 days, and a mean lifetime of 24 days.

There is an important qualification to bear in mind here, though: these plots only show regressions which have been fixed. Over the year or so surveyed, 858 regressions were reported, but only 738 of those were reported to be fixed. So, says Rafael, the full picture has to include a second type of regression which is harder to find and to fix.

What would really help, it was agreed, is more testers - hardly a new or novel conclusion. 


Perhaps if more people would run linux-next, more bugs would be found (and fixed) early. Linux-next is hard to test, though, and hard to debug. It changes radically from one day to the next, and it is not bisectable.

Andrew Morton thought that developers should be testing their code in linux-next as a way of finding bugs caused by interactions with other changes. That is a hard sell, though; working with linux-next can force developers to contend with bugs from completely unrelated changes. Alan Cox noted that linux-next is often more reliable than the -rc1 releases.    

There was some talk about how long it can take for regression fixes to get into the mainline. Often these fixes will set in maintainer trees for some time. Might there be a way to get them in more quickly? As it turns out, a number of subsystem maintainers feel that these fixes should age for a while in a testing tree, lest they introduce other problems. So that situation is not likely to change much.


We *have* that. The problem is that users tend to have stranger hardware 
and stranger configurations than anything that we can actually *test*: 
this is as opposed to Ingo's randconfig does-it-boot testing


[KS2010: Regressions]

How much better is not clear, though. According to Rafael, the mean lifetime of a reported regression is "approximately 24.5 days." That number has not really changed over the last two years.

Where are the regressions being found? The direct rendering subsystem is by far the biggest offender, with the Intel-specific DRI code, on its own, being at the top of the list. After that are the x86 architecture, the filesystem code, and the network subsystem. This information led to the fairly obvious conclusion that regressions tend to be found in code which (1) is actually used, and (2) is under relatively heavy development. Rafael also noted that code which deals closely with hardware tends to be more prone to regressions than core kernel code.

Rafael took some time to complain about the state of the kernel bugzilla system which, it seems, is essentially unmaintained at the moment. Sometimes it just doesn't work, it doesn't track information he would like to have, an account is required for anybody who would like to be on the Cc list, etc. So it seems that there is an opening available for somebody who would like to volunteer to improve the kernel bugzilla; volunteers were notably absent from the room, though.

ã€KS2011: Regressionsã€‘
Rafael Wysocki's talk on regression tracking has become a kernel summit tradition. The overall picture remains quite stable; most kernels have 20-30 outstanding regressions one full cycle after their release. The number of short-term regressions seems to have fallen a bit in recent times; Rafael chalked that up to the fact that the graphics drivers have finally stabilized somewhat.The number of short-term regressions seems to have fallen a bit in recent times


The numbers showed that, as a whole, regressions are not being found as quickly as they were a year or so ago. It was asked: is that good or bad? Perhaps it is bad; we're just not as good at finding bugs as we once were. Linus, instead, said that it's a sign that the bugs we are seeing are becoming more subtle; they don't affect as many users, and, thus, take more time to find. Rafael thought it may mean that we were getting more testing of post-release kernels than before, but he wasn't sure. He was convinced that the trend meant something, but couldn't say what.

[A more detailed look at kernel regressions]

å®šä¹‰

A regression is a user-visible change in the behavior of the kernel between two releases.

A program that was working on one kernel version and then suddenly stops working on a newer version has detected a kernel regression. 


Regressions are added to bugzilla one week after they are reported by email, if they haven't been fixed the interim. That's a change from earlier practices to save Rutecki's time as well as to reduce unhelpful noise. Bugzilla entries are linked to fixes as they become available. The bug state is changed to "resolved" once a patch is available and "closed" once Torvalds merges the fix into the mainline.


å³°å…‰å…³æ³¨äº†mailä¸­æåˆ°çš„bug reportså—